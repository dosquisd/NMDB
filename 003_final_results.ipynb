{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4a162b",
   "metadata": {},
   "source": [
    "# Final Results\n",
    "\n",
    "Recalculate metrics and plot them, but done at many more stations for each datetime. Basically, a more robust version of [002](./002_calc_some_metrics.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3593cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import TypedDict, Optional\n",
    "\n",
    "from random import sample\n",
    "from utils.constants import WINDOW_SIZE, Events, NAN_THRESHOLD, EWM_ALPHA, METRICS\n",
    "from utils import (\n",
    "    load_data,\n",
    "    calc_metrics,\n",
    "    plot_metrics,\n",
    "    read_metrics_file,\n",
    "    plot_metrics_one,\n",
    ")\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c51148",
   "metadata": {},
   "outputs": [],
   "source": [
    "DatetimeBounds = list[str, str]\n",
    "\n",
    "\n",
    "class DateEventsInfo(TypedDict):\n",
    "    \"\"\"\n",
    "    Information about the event date.\n",
    "\n",
    "    Attributes:\n",
    "        bounds: list[DatetimeBounds]. The start and end bounds of the event date. Generally,\n",
    "                the bounds are the same for all stations.\n",
    "        freq: str. The frequency of the data for the event date.\n",
    "        stations: dict[str, Optional[DatetimeBounds]]. The list of relevant stations\n",
    "                  for the event date.\n",
    "    \"\"\"\n",
    "\n",
    "    bounds: list[DatetimeBounds]\n",
    "    freq: str\n",
    "    stations: dict[str, Optional[DatetimeBounds]]\n",
    "\n",
    "\n",
    "class StationsToChoose(TypedDict):\n",
    "    \"\"\"\n",
    "    Structure for station selection data.\n",
    "\n",
    "    Attributes:\n",
    "        stations: list[str]. Available station names for random sampling.\n",
    "        num_sample: int. Number of stations to sample from the available list.\n",
    "    \"\"\"\n",
    "\n",
    "    stations: list[str]\n",
    "    num_sample: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac7bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT: Events = \"Forbush Decrease\"\n",
    "MAX_SAMPLES: int = 10  # Samples per date\n",
    "REPETITION: bool = True  # If True, it will repeat stations already calculated\n",
    "EWM: bool = True  # If True, it will calculate EWM metrics\n",
    "\n",
    "event_replace: str = EVENT.replace(\" \", \"\")\n",
    "\n",
    "# Relevant dates for the event\n",
    "# Stations lists are only examples where the event was clear;\n",
    "# can be modify them as needed\n",
    "# TODO: Fix datetime event for each station\n",
    "datetimes: dict[str, Optional[DateEventsInfo]] = {\n",
    "    \"2023-04-23\": {\n",
    "        \"bounds\": [\"2023-04-23 23:00:00\", \"2023-04-24 06:00:00\"],\n",
    "        \"freq\": \"1h\",\n",
    "        \"stations\": {\n",
    "            \"AATB\": None,\n",
    "            \"APTY\": None,\n",
    "            \"IRK2\": None,\n",
    "            \"LMKS\": None,\n",
    "            \"NEWK\": None,\n",
    "            \"NAIN\": None,\n",
    "            \"SOPO\": None,\n",
    "        },\n",
    "    },\n",
    "    \"2024-03-24\": {\n",
    "        \"bounds\": [\"2024-03-24 14:00:00\", \"2024-03-25 04:30:00\"],\n",
    "        \"freq\": \"90min\",\n",
    "        \"stations\": {\n",
    "            \"APTY\": None,\n",
    "            \"DOMC\": None,\n",
    "            \"INVK\": None,\n",
    "            \"JUNG1\": None,\n",
    "            \"KIEL2\": None,\n",
    "            \"LMKS\": None,\n",
    "            \"MWSN\": None,\n",
    "            \"NEWK\": None,\n",
    "            \"MXCO\": None,\n",
    "            \"OULU\": None,\n",
    "            # TXBY, YKTK\n",
    "        },\n",
    "    },\n",
    "    \"2024-05-10\": {\n",
    "        \"bounds\": [\"2024-05-10 18:00:00\", \"2024-05-11 01:00:00\"],\n",
    "        \"freq\": \"1h\",\n",
    "        \"stations\": {\n",
    "            \"APTY\": None,\n",
    "            \"DOMB\": None,\n",
    "            \"DOMC\": None,\n",
    "            \"INVK\": None,\n",
    "            \"IRK3\": None,\n",
    "            \"JBGO\": None,\n",
    "            \"KERG\": None,\n",
    "            \"KIEL2\": None,\n",
    "            \"LMKS\": None,\n",
    "            \"MWSN\": None,\n",
    "            # SOPB, PWNK, SOPO, TERA, THUL, TXBY, YKTK\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed2641e",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b489ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_stations(\n",
    "    df: pd.DataFrame, threshold: Optional[float] = None\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Drop stations (columns) that have a high ratio of NaN values.\n",
    "    The DataFrame is the one with the data of the stations (all.txt).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with the stations data.\n",
    "        threshold (Optional[float]): Ratio of NaN values to consider a station invalid.\n",
    "            If None, it will use the global NAN_THRESHOLD.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of valid station names (columns).\n",
    "    \"\"\"\n",
    "    if threshold is None:\n",
    "        threshold = NAN_THRESHOLD\n",
    "\n",
    "    nans_count = dict(\n",
    "        filter(\n",
    "            lambda x: x[1] > 0,\n",
    "            df.drop(columns=\"datetime\").isna().sum().to_dict().items(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    total = len(df)\n",
    "\n",
    "    # Drop columns (stations) that superates a nan ratio threshold\n",
    "    stations = list(\n",
    "        df.drop(\n",
    "            columns=list(\n",
    "                filter(\n",
    "                    lambda station: nans_count[station] / total >= threshold, nans_count\n",
    "                )\n",
    "            )\n",
    "        ).columns[1:]\n",
    "    )\n",
    "\n",
    "    return stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5822326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations: dict[str, list[str]] = {\n",
    "    date: get_valid_stations(\n",
    "        load_data(f\"./data/{event_replace}/{date}/all.txt\"),\n",
    "        threshold=NAN_THRESHOLD,\n",
    "    )\n",
    "    for date in datetimes\n",
    "}\n",
    "\n",
    "choosen_stations: dict[str, list[str]] = {\n",
    "    date: list(\n",
    "        map(\n",
    "            lambda filename: filename.name.strip().split(\"_\", 1)[0].upper(),\n",
    "            Path(f\"./data/{event_replace}/{date}\").glob(\"*.csv\"),\n",
    "        )\n",
    "    )\n",
    "    for date in datetimes\n",
    "}\n",
    "\n",
    "stations_to_choose: dict[str, StationsToChoose] = {\n",
    "    date: {\n",
    "        # Remove stations already calculated\n",
    "        # and those that are fixed to be calculated\n",
    "        \"stations\": list(\n",
    "            set(stations[date])\n",
    "            - set(choosen_stations[date])\n",
    "            - set(datetimes[date][\"stations\"].keys())\n",
    "        ),\n",
    "        # Final number of samples to choose\n",
    "        \"num_sample\": num_samples\n",
    "        if (\n",
    "            num_samples := MAX_SAMPLES\n",
    "            - len(choosen_stations[date])\n",
    "            - len(datetimes[date][\"stations\"].keys())\n",
    "        )\n",
    "        > 0\n",
    "        else 0,\n",
    "    }\n",
    "    for date in datetimes\n",
    "}\n",
    "\n",
    "# Without repetition of stations already calculated\n",
    "plot_stations = {\n",
    "    date: sample(items[\"stations\"], k=items[\"num_sample\"])\n",
    "    + list(datetimes[date][\"stations\"].keys())\n",
    "    for date, items in stations_to_choose.items()\n",
    "}\n",
    "\n",
    "# Here is added repetition of stations already calculated (if needed)\n",
    "if REPETITION:\n",
    "    for date in plot_stations:\n",
    "        plot_stations[date].extend(choosen_stations[date])\n",
    "\n",
    "        # Drop duplicates\n",
    "        plot_stations[date] = list(set(plot_stations[date]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae4968e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 minutes approximate to calculate all metrics with my pc\n",
    "suffix = f\"ewm_alpha_{EWM_ALPHA}\" if EWM_ALPHA else \"\"\n",
    "\n",
    "\n",
    "def prepare_df(path: str) -> pd.DataFrame:\n",
    "    df = load_data(path).set_index(\"datetime\")\n",
    "    return df.ewm(alpha=EWM_ALPHA).mean() if EWM_ALPHA and EWM else df\n",
    "\n",
    "\n",
    "arguments = [\n",
    "    (\n",
    "        prepare_df(f\"./data/{event_replace}/{date}/all.txt\"),\n",
    "        station,\n",
    "        date,\n",
    "        suffix,\n",
    "    )\n",
    "    for date, stations in plot_stations.items()\n",
    "    for station in stations\n",
    "]\n",
    "\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    results = pool.starmap(\n",
    "        calc_metrics,\n",
    "        arguments,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a61c8e",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "847902fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stations: dict[str, list[str]] = {\n",
    "    date: list(\n",
    "        set(\n",
    "            map(\n",
    "                # Get Station name from filename\n",
    "                lambda filename: filename.name.strip().split(\"_\", 1)[0].upper(),\n",
    "                Path(f\"./data/{event_replace}/{date}\").glob(\"*.csv\"),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    for date in datetimes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd202280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2023-04-23': 10, '2024-03-24': 11, '2024-05-10': 11}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nice! Expected output\n",
    "dict(map(lambda x: (x, len(plot_stations[x])), plot_stations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0832522b",
   "metadata": {},
   "source": [
    "### Two differents plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36e9fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_wrapper(args_tuple: tuple[str, str, int]) -> None:\n",
    "    date, station, suffix = args_tuple\n",
    "\n",
    "    df = read_metrics_file(\n",
    "        event=event_replace,\n",
    "        date=date,\n",
    "        station=station,\n",
    "        window_size=WINDOW_SIZE,\n",
    "        datetime_cols={\"datetime\": \"\"},\n",
    "        suffix=f\"-ewm_alpha_{EWM_ALPHA}\" if EWM_ALPHA and EWM else \"\",\n",
    "    )\n",
    "\n",
    "    # Odd suffix: all metrics except \"lepel_ziv\"\n",
    "    if suffix % 2 == 1:\n",
    "        df = df.drop(columns=[\"lepel_ziv\"], errors=\"ignore\")\n",
    "        relevant_metrics = [\"*\"]\n",
    "\n",
    "    else:  # Even suffix: only \"lepel_ziv\"\n",
    "        relevant_metrics = [\"lepel_ziv\"]\n",
    "\n",
    "    if station in datetimes[date][\"stations\"] and datetimes[date][\"stations\"][station]:\n",
    "        min_datetime, max_datetime = datetimes[date][\"stations\"][station]\n",
    "    else:\n",
    "        min_datetime, max_datetime = datetimes[date][\"bounds\"]\n",
    "\n",
    "    plot_metrics(\n",
    "        window_size=WINDOW_SIZE,\n",
    "        relevant_metrics=relevant_metrics,\n",
    "        df=df,\n",
    "        event=event_replace,\n",
    "        date=date,\n",
    "        station=station,\n",
    "        min_datetime=min_datetime,\n",
    "        max_datetime=max_datetime,\n",
    "        freq_date_range=datetimes[date][\"freq\"],\n",
    "        save_format=\"pdf\",\n",
    "        suffix=str(suffix),\n",
    "        show=False,\n",
    "    )\n",
    "\n",
    "\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    arguments_plot = [\n",
    "        (date, station, suffix + 2 if EWM else suffix)\n",
    "        for date, stations in plot_stations.items()\n",
    "        for station in stations\n",
    "        for suffix in [1, 2]\n",
    "    ]\n",
    "\n",
    "    pool.map(plot_metrics_wrapper, arguments_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc594f3",
   "metadata": {},
   "source": [
    "### One plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db4daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_one_wrapper(args_tuple: tuple[str, str]) -> None:\n",
    "    date, station = args_tuple\n",
    "\n",
    "    df = read_metrics_file(\n",
    "        event=event_replace,\n",
    "        date=date,\n",
    "        station=station,\n",
    "        window_size=WINDOW_SIZE,\n",
    "        datetime_cols={\"datetime\": \"\"},\n",
    "        suffix=f\"-ewm_alpha_{EWM_ALPHA}\" if EWM_ALPHA and EWM else \"\",\n",
    "    )\n",
    "\n",
    "    if station in datetimes[date][\"stations\"] and datetimes[date][\"stations\"][station]:\n",
    "        min_datetime, max_datetime = datetimes[date][\"stations\"][station]\n",
    "    else:\n",
    "        min_datetime, max_datetime = datetimes[date][\"bounds\"]\n",
    "\n",
    "    display(df.head())\n",
    "\n",
    "    plot_metrics_one(\n",
    "        window_size=WINDOW_SIZE,\n",
    "        relevant_metrics=None,\n",
    "        df=df,\n",
    "        event=event_replace,\n",
    "        date=date,\n",
    "        station=station,\n",
    "        min_datetime=min_datetime,\n",
    "        max_datetime=max_datetime,\n",
    "        freq_date_range=datetimes[date][\"freq\"],\n",
    "        rotation_xticks=60,\n",
    "        save_format=\"pdf\",\n",
    "        show=False,\n",
    "    )\n",
    "\n",
    "\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    arguments_plot = [\n",
    "        (date, station)\n",
    "        for date, stations in plot_stations.items()\n",
    "        for station in stations\n",
    "    ]\n",
    "\n",
    "    pool.map(plot_metrics_one_wrapper, arguments_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097cc45f",
   "metadata": {},
   "source": [
    "## Prediction with derivates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe9684ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsSummary(TypedDict):\n",
    "    \"\"\"\n",
    "    Structure for metrics summary data.\n",
    "\n",
    "    Attributes:\n",
    "        event (str): The name of the event.\n",
    "        date (str): The date of the event.\n",
    "        station (str): The station name.\n",
    "        metric (str): The name of the calculated metric.\n",
    "        index (str): The calculated metrics for the station.\n",
    "    \"\"\"\n",
    "\n",
    "    event: str\n",
    "    date: str\n",
    "    station: str\n",
    "    metric: str\n",
    "    index: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30376c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stations: dict[str, list[str]] = {\n",
    "    date: list(\n",
    "        set(\n",
    "            map(\n",
    "                # Get Station name from filename\n",
    "                lambda filename: filename.name.strip().split(\"_\", 1)[0].upper(),\n",
    "                Path(f\"./data/{event_replace}/{date}\").glob(\"*.csv\"),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    for date in datetimes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e2e3a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_interval(\n",
    "    event: Events,\n",
    "    date: str,\n",
    "    station: str,\n",
    "    data: pd.DataFrame = None,\n",
    ") -> pd.DataFrame:\n",
    "    if data is None:\n",
    "        suffix = f\"-ewm_alpha_{EWM_ALPHA}\" if EWM_ALPHA and EWM else \"\"\n",
    "        data = read_metrics_file(\n",
    "            event,\n",
    "            date,\n",
    "            station,\n",
    "            WINDOW_SIZE,\n",
    "            datetime_cols={\"datetime\": None},\n",
    "            suffix=suffix,\n",
    "        ).set_index(\"datetime\")\n",
    "\n",
    "    if station in datetimes[date][\"stations\"] and datetimes[date][\"stations\"][station]:\n",
    "        max_datetime = datetimes[date][\"stations\"][station][1]\n",
    "    else:\n",
    "        max_datetime = datetimes[date][\"bounds\"][1]\n",
    "\n",
    "    data = data[(data[\"window_shape\"] == WINDOW_SIZE) & (data.index <= max_datetime)]\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_derivatives(\n",
    "    event: Events, date: str, station: str, percentil: int = 0.95\n",
    ") -> list[MetricsSummary]:\n",
    "    assert 0 < percentil < 1.0, \"Percentil must be between 0.0 and 1.0\"\n",
    "\n",
    "    suffix = f\"-ewm_alpha_{EWM_ALPHA}\" if EWM_ALPHA and EWM else \"\"\n",
    "    data = read_metrics_file(\n",
    "        event,\n",
    "        date,\n",
    "        station,\n",
    "        WINDOW_SIZE,\n",
    "        datetime_cols={\"datetime\": None},\n",
    "        suffix=suffix,\n",
    "    ).set_index(\"datetime\")\n",
    "\n",
    "    metrics_columns = list(filter(lambda col: col in METRICS, data.columns))\n",
    "    metrics_columns += [\"value\"]\n",
    "\n",
    "    valid_indexes = valid_interval(event, date, station, data).index\n",
    "    diff = data[metrics_columns].diff()\n",
    "    interest_df = diff[diff.index.isin(valid_indexes)]\n",
    "    quantiles = interest_df.quantile(percentil)\n",
    "\n",
    "    results: list[MetricsSummary] = []\n",
    "    for col in metrics_columns:\n",
    "        quantil = quantiles[col]\n",
    "        points = interest_df[interest_df[col] >= quantil][col]\n",
    "        if len(points) < 0:\n",
    "            continue\n",
    "\n",
    "        interest_index = points.idxmax()  # Maybe this operation can be changed\n",
    "        results.append(\n",
    "            {\n",
    "                \"event\": event,\n",
    "                \"date\": date,\n",
    "                \"station\": station,\n",
    "                \"metric\": col,\n",
    "                \"index\": str(interest_index),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d6d840df",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentil: float = 0.9\n",
    "\n",
    "arguments: list[tuple[Events, str, str, int]] = list(\n",
    "    map(\n",
    "        lambda date, station: (\"Forbush Decrease\", date, station, percentil),\n",
    "        *zip(\n",
    "            *[\n",
    "                (date, station)\n",
    "                for date, stations in plot_stations.items()\n",
    "                for station in stations\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    results = pool.starmap(\n",
    "        process_derivatives,\n",
    "        arguments,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "84e6e080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 15)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to check => Resulting DataFrame should have 480 rows\n",
    "len(results), len(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c634a639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "station",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "metric",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "index",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "event",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a4a652ba-6ea6-4370-99b7-3996cc161ac0",
       "rows": [
        [
         "0",
         "2023-04-23 00:00:00",
         "NEWK",
         "entropy",
         "2023-04-23 01:05:00",
         "Forbush Decrease"
        ],
        [
         "1",
         "2023-04-23 00:00:00",
         "NEWK",
         "sampen",
         "2023-04-23 11:44:00",
         "Forbush Decrease"
        ],
        [
         "2",
         "2023-04-23 00:00:00",
         "NEWK",
         "permutation_entropy",
         "2023-04-24 02:54:00",
         "Forbush Decrease"
        ],
        [
         "3",
         "2023-04-23 00:00:00",
         "NEWK",
         "shannon_entropy",
         "2023-04-23 18:37:00",
         "Forbush Decrease"
        ],
        [
         "4",
         "2023-04-23 00:00:00",
         "NEWK",
         "spectral_entropy",
         "2023-04-24 02:59:00",
         "Forbush Decrease"
        ],
        [
         "5",
         "2023-04-23 00:00:00",
         "NEWK",
         "app_entropy",
         "2023-04-24 05:41:00",
         "Forbush Decrease"
        ],
        [
         "6",
         "2023-04-23 00:00:00",
         "NEWK",
         "hurst",
         "2023-04-23 21:22:00",
         "Forbush Decrease"
        ],
        [
         "7",
         "2023-04-23 00:00:00",
         "NEWK",
         "dfa",
         "2023-04-23 22:25:00",
         "Forbush Decrease"
        ],
        [
         "8",
         "2023-04-23 00:00:00",
         "NEWK",
         "mfhurst_b",
         "2023-04-23 20:25:00",
         "Forbush Decrease"
        ],
        [
         "9",
         "2023-04-23 00:00:00",
         "NEWK",
         "higuchi_fd",
         "2023-04-23 09:18:00",
         "Forbush Decrease"
        ],
        [
         "10",
         "2023-04-23 00:00:00",
         "NEWK",
         "katz_fd",
         "2023-04-23 09:47:00",
         "Forbush Decrease"
        ],
        [
         "11",
         "2023-04-23 00:00:00",
         "NEWK",
         "petrosian_fd",
         "2023-04-23 20:04:00",
         "Forbush Decrease"
        ],
        [
         "12",
         "2023-04-23 00:00:00",
         "NEWK",
         "lepel_ziv",
         "2023-04-23 07:30:00",
         "Forbush Decrease"
        ],
        [
         "13",
         "2023-04-23 00:00:00",
         "NEWK",
         "corr_dim",
         "2023-04-23 10:40:00",
         "Forbush Decrease"
        ],
        [
         "14",
         "2023-04-23 00:00:00",
         "NEWK",
         "value",
         "2023-04-23 08:42:00",
         "Forbush Decrease"
        ],
        [
         "15",
         "2023-04-23 00:00:00",
         "AATB",
         "entropy",
         "2023-04-23 01:05:00",
         "Forbush Decrease"
        ],
        [
         "16",
         "2023-04-23 00:00:00",
         "AATB",
         "sampen",
         "2023-04-23 07:26:00",
         "Forbush Decrease"
        ],
        [
         "17",
         "2023-04-23 00:00:00",
         "AATB",
         "permutation_entropy",
         "2023-04-23 20:51:00",
         "Forbush Decrease"
        ],
        [
         "18",
         "2023-04-23 00:00:00",
         "AATB",
         "shannon_entropy",
         "2023-04-23 02:20:00",
         "Forbush Decrease"
        ],
        [
         "19",
         "2023-04-23 00:00:00",
         "AATB",
         "spectral_entropy",
         "2023-04-23 17:01:00",
         "Forbush Decrease"
        ],
        [
         "20",
         "2023-04-23 00:00:00",
         "AATB",
         "app_entropy",
         "2023-04-24 04:20:00",
         "Forbush Decrease"
        ],
        [
         "21",
         "2023-04-23 00:00:00",
         "AATB",
         "hurst",
         "2023-04-23 04:06:00",
         "Forbush Decrease"
        ],
        [
         "22",
         "2023-04-23 00:00:00",
         "AATB",
         "dfa",
         "2023-04-23 14:48:00",
         "Forbush Decrease"
        ],
        [
         "23",
         "2023-04-23 00:00:00",
         "AATB",
         "mfhurst_b",
         "2023-04-23 02:25:00",
         "Forbush Decrease"
        ],
        [
         "24",
         "2023-04-23 00:00:00",
         "AATB",
         "higuchi_fd",
         "2023-04-23 19:16:00",
         "Forbush Decrease"
        ],
        [
         "25",
         "2023-04-23 00:00:00",
         "AATB",
         "katz_fd",
         "2023-04-23 07:40:00",
         "Forbush Decrease"
        ],
        [
         "26",
         "2023-04-23 00:00:00",
         "AATB",
         "petrosian_fd",
         "2023-04-23 18:35:00",
         "Forbush Decrease"
        ],
        [
         "27",
         "2023-04-23 00:00:00",
         "AATB",
         "lepel_ziv",
         "2023-04-23 01:51:00",
         "Forbush Decrease"
        ],
        [
         "28",
         "2023-04-23 00:00:00",
         "AATB",
         "corr_dim",
         "2023-04-23 14:24:00",
         "Forbush Decrease"
        ],
        [
         "29",
         "2023-04-23 00:00:00",
         "AATB",
         "value",
         "2023-04-23 18:35:00",
         "Forbush Decrease"
        ],
        [
         "30",
         "2023-04-23 00:00:00",
         "DOMB",
         "entropy",
         "2023-04-23 01:05:00",
         "Forbush Decrease"
        ],
        [
         "31",
         "2023-04-23 00:00:00",
         "DOMB",
         "sampen",
         "2023-04-24 03:39:00",
         "Forbush Decrease"
        ],
        [
         "32",
         "2023-04-23 00:00:00",
         "DOMB",
         "permutation_entropy",
         "2023-04-23 01:49:00",
         "Forbush Decrease"
        ],
        [
         "33",
         "2023-04-23 00:00:00",
         "DOMB",
         "shannon_entropy",
         "2023-04-23 10:38:00",
         "Forbush Decrease"
        ],
        [
         "34",
         "2023-04-23 00:00:00",
         "DOMB",
         "spectral_entropy",
         "2023-04-23 09:36:00",
         "Forbush Decrease"
        ],
        [
         "35",
         "2023-04-23 00:00:00",
         "DOMB",
         "app_entropy",
         "2023-04-23 13:39:00",
         "Forbush Decrease"
        ],
        [
         "36",
         "2023-04-23 00:00:00",
         "DOMB",
         "hurst",
         "2023-04-23 01:25:00",
         "Forbush Decrease"
        ],
        [
         "37",
         "2023-04-23 00:00:00",
         "DOMB",
         "dfa",
         "2023-04-24 03:40:00",
         "Forbush Decrease"
        ],
        [
         "38",
         "2023-04-23 00:00:00",
         "DOMB",
         "mfhurst_b",
         "2023-04-23 01:53:00",
         "Forbush Decrease"
        ],
        [
         "39",
         "2023-04-23 00:00:00",
         "DOMB",
         "higuchi_fd",
         "2023-04-23 09:46:00",
         "Forbush Decrease"
        ],
        [
         "40",
         "2023-04-23 00:00:00",
         "DOMB",
         "katz_fd",
         "2023-04-23 12:50:00",
         "Forbush Decrease"
        ],
        [
         "41",
         "2023-04-23 00:00:00",
         "DOMB",
         "petrosian_fd",
         "2023-04-23 01:12:00",
         "Forbush Decrease"
        ],
        [
         "42",
         "2023-04-23 00:00:00",
         "DOMB",
         "lepel_ziv",
         "2023-04-23 01:36:00",
         "Forbush Decrease"
        ],
        [
         "43",
         "2023-04-23 00:00:00",
         "DOMB",
         "corr_dim",
         "2023-04-23 11:58:00",
         "Forbush Decrease"
        ],
        [
         "44",
         "2023-04-23 00:00:00",
         "DOMB",
         "value",
         "2023-04-23 20:08:00",
         "Forbush Decrease"
        ],
        [
         "45",
         "2023-04-23 00:00:00",
         "SOPO",
         "entropy",
         "2023-04-23 01:05:00",
         "Forbush Decrease"
        ],
        [
         "46",
         "2023-04-23 00:00:00",
         "SOPO",
         "sampen",
         "2023-04-23 08:24:00",
         "Forbush Decrease"
        ],
        [
         "47",
         "2023-04-23 00:00:00",
         "SOPO",
         "permutation_entropy",
         "2023-04-24 01:13:00",
         "Forbush Decrease"
        ],
        [
         "48",
         "2023-04-23 00:00:00",
         "SOPO",
         "shannon_entropy",
         "2023-04-24 00:49:00",
         "Forbush Decrease"
        ],
        [
         "49",
         "2023-04-23 00:00:00",
         "SOPO",
         "spectral_entropy",
         "2023-04-24 00:59:00",
         "Forbush Decrease"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 480
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>station</th>\n",
       "      <th>metric</th>\n",
       "      <th>index</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-23</td>\n",
       "      <td>NEWK</td>\n",
       "      <td>entropy</td>\n",
       "      <td>2023-04-23 01:05:00</td>\n",
       "      <td>Forbush Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-23</td>\n",
       "      <td>NEWK</td>\n",
       "      <td>sampen</td>\n",
       "      <td>2023-04-23 11:44:00</td>\n",
       "      <td>Forbush Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-23</td>\n",
       "      <td>NEWK</td>\n",
       "      <td>permutation_entropy</td>\n",
       "      <td>2023-04-24 02:54:00</td>\n",
       "      <td>Forbush Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-23</td>\n",
       "      <td>NEWK</td>\n",
       "      <td>shannon_entropy</td>\n",
       "      <td>2023-04-23 18:37:00</td>\n",
       "      <td>Forbush Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-23</td>\n",
       "      <td>NEWK</td>\n",
       "      <td>spectral_entropy</td>\n",
       "      <td>2023-04-24 02:59:00</td>\n",
       "      <td>Forbush Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>2024-05-10</td>\n",
       "      <td>CALG</td>\n",
       "      <td>katz_fd</td>\n",
       "      <td>2024-05-10 02:35:00</td>\n",
       "      <td>Forbush Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>2024-05-10</td>\n",
       "      <td>CALG</td>\n",
       "      <td>petrosian_fd</td>\n",
       "      <td>2024-05-10 19:45:00</td>\n",
       "      <td>Forbush Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>2024-05-10</td>\n",
       "      <td>CALG</td>\n",
       "      <td>lepel_ziv</td>\n",
       "      <td>2024-05-10 02:59:00</td>\n",
       "      <td>Forbush Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>2024-05-10</td>\n",
       "      <td>CALG</td>\n",
       "      <td>corr_dim</td>\n",
       "      <td>2024-05-10 05:47:00</td>\n",
       "      <td>Forbush Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>2024-05-10</td>\n",
       "      <td>CALG</td>\n",
       "      <td>value</td>\n",
       "      <td>2024-05-10 20:20:00</td>\n",
       "      <td>Forbush Decrease</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date station               metric               index  \\\n",
       "0   2023-04-23    NEWK              entropy 2023-04-23 01:05:00   \n",
       "1   2023-04-23    NEWK               sampen 2023-04-23 11:44:00   \n",
       "2   2023-04-23    NEWK  permutation_entropy 2023-04-24 02:54:00   \n",
       "3   2023-04-23    NEWK      shannon_entropy 2023-04-23 18:37:00   \n",
       "4   2023-04-23    NEWK     spectral_entropy 2023-04-24 02:59:00   \n",
       "..         ...     ...                  ...                 ...   \n",
       "475 2024-05-10    CALG              katz_fd 2024-05-10 02:35:00   \n",
       "476 2024-05-10    CALG         petrosian_fd 2024-05-10 19:45:00   \n",
       "477 2024-05-10    CALG            lepel_ziv 2024-05-10 02:59:00   \n",
       "478 2024-05-10    CALG             corr_dim 2024-05-10 05:47:00   \n",
       "479 2024-05-10    CALG                value 2024-05-10 20:20:00   \n",
       "\n",
       "                event  \n",
       "0    Forbush Decrease  \n",
       "1    Forbush Decrease  \n",
       "2    Forbush Decrease  \n",
       "3    Forbush Decrease  \n",
       "4    Forbush Decrease  \n",
       "..                ...  \n",
       "475  Forbush Decrease  \n",
       "476  Forbush Decrease  \n",
       "477  Forbush Decrease  \n",
       "478  Forbush Decrease  \n",
       "479  Forbush Decrease  \n",
       "\n",
       "[480 rows x 5 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"date\", \"station\", \"metric\", \"index\"])\n",
    "for res in results:\n",
    "    df = pd.concat([df, pd.DataFrame(res)], ignore_index=True)\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"index\"] = pd.to_datetime(df[\"index\"])\n",
    "\n",
    "df.to_csv(f\"./data/{event_replace}/summary_derivatives.csv\", index=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc33c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
